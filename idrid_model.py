# -*- coding: utf-8 -*-
"""IDRiD_SMALL_PATCH_December_3_UNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lst17QG1FQHwu4vxXOrQ3oNqSVzxdQyC
"""


import zipfile
import shutil
from pathlib import Path
from sklearn.model_selection import train_test_split
import random
from PIL import Image
import numpy as np
from collections import defaultdict
import torch
import os
import cv2
import re

import glob
from torchvision import transforms, datasets
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms.functional as TF
import matplotlib.pyplot as plt

#from __future__ import annotations
import math
from dataclasses import dataclass
from typing import Tuple, Dict, Any

import torch.nn as nn
import torch.nn.functional as F

from torch.cuda.amp import autocast, GradScaler
import math
import time
from sklearn.metrics import average_precision_score
from torch.serialization import add_safe_globals

'''
#Open the Zipfile
with zipfile.ZipFile('A. Segmentation.zip', 'r') as zip_ref:
    # List all contents
    file_list = zip_ref.namelist()
    print("Contents of the ZIP file:")
    for file in file_list:
        print(file)
'''

"""Apply Augmentations"""

class RetinaSegDataset(Dataset):
    """
    Loads one RGB image and its 4 lesion masks as a 4-channel tensor (C=4,H,W).
    If augment=True: applies the SAME random crop/affine/flip to image AND masks.
    """
    def __init__(
        self,
        img_dir: str,
        mask_root: str,
        lesion_subdirs = ("Combined MA", "Combined HE", "Combined SE", "Combined EX", "Combined OD"),
        image_size: int | tuple = 256,
        augment: bool = True,
        degrees: float = 30,
        translate=(0.10, 0.10),
        scale_range=(0.9, 1.1),
        shear_range=(-15, 15),
        rr_crop_scale=(0.9, 1.0),
        rr_crop_ratio=(0.75, 1.333),
        normalize_mean=(0.485, 0.456, 0.406),
        normalize_std=(0.229, 0.224, 0.225),
        crop_left_frac: float = 0.055,
        crop_right_frac: float = 0.013,
        per_image_crop: dict | None = None,

        # JUST ADDED -> patch-based training options
        patch_mode: bool = False,
        patches_per_image: int = 1,
    ):
        self.img_dir = Path(img_dir)
        self.mask_root = Path(mask_root)
        self.lesion_subdirs = lesion_subdirs
        self.image_size = (image_size, image_size) if isinstance(image_size, int) else tuple(image_size)
        self.augment = augment
        self.crop_left_frac = crop_left_frac
        self.crop_right_frac = crop_right_frac
        self.per_image_crop = per_image_crop or {}

        # JUST ADDED -> patch-based options
        self.patch_mode = patch_mode          # if True: thus, random 512×512 patches
        self.patches_per_image = max(1, patches_per_image)

        # augmentation params
        self.degrees = degrees
        self.translate = translate
        self.scale_range = scale_range
        self.shear_range = shear_range
        self.rr_crop_scale = rr_crop_scale
        self.rr_crop_ratio = rr_crop_ratio

        # collect images (any common image extension)
        self.img_paths = sorted(
            sum([list(self.img_dir.glob(f"*.{ext}")) for ext in ["jpg","jpeg","png","tif","tiff","bmp"]], [])
        )
        if len(self.img_paths) == 0:
            raise FileNotFoundError(f"No images found in {self.img_dir}")

        # verify all lesion folders exist
        for sub in self.lesion_subdirs:
            if not (self.mask_root / sub).exists():
                raise FileNotFoundError(f"Missing mask subfolder: {self.mask_root / sub}")

        self.normalize = transforms.Normalize(mean=normalize_mean, std=normalize_std)

    def __len__(self):
        # In patch mode, we conceptually replicate each image P times per epoch.
        if self.patch_mode and self.patches_per_image > 1:
            return len(self.img_paths) * self.patches_per_image
        return len(self.img_paths)

    def _find_mask_path(self, subdir: Path, stem: str):
        # try multiple extensions for masks
        for ext in ("tif","tiff","png","jpg","jpeg","bmp"):
            p = subdir / f"{stem}.{ext}"
            if p.exists():
                return p
        # fallback: glob by stem.*
        hits = glob.glob(str(subdir / f"{stem}.*"))
        if len(hits) > 0:
            return Path(hits[0])
        # CHANGE: return None instead of raising
        return None

    def _load_image(self, p: Path):
        # PIL can read TIFF; for LZW you may need imagecodecs (you already installed earlier)
        return Image.open(p).convert("RGB")

    def _load_mask_as_binary_L(self, p: Path):
        # convert to single-channel and binarize (>0 -> 1)
        m = Image.open(p).convert("L")
        # ensure binary (some masks are 0/255 or multi-gray); threshold at >0
        m = m.point(lambda x: 255 if x > 0 else 0)
        return m

    def __getitem__(self, idx):
        # In patch_mode, many dataset indices map to the same image,
        # but we draw a fresh random patch each time.
        if self.patch_mode:
            img_idx = idx % len(self.img_paths)
        else:
            img_idx = idx

        img_path = self.img_paths[img_idx]
        stem = img_path.stem

        # load image
        img = self._load_image(img_path)

        # load 4 lesion masks in a fixed order
        mask_imgs = []
        present = []
        for sub in self.lesion_subdirs:
            mp = self._find_mask_path(self.mask_root / sub, stem)
            if mp is None:
                # mark missing; make a blank mask same size as the image
                present.append(0.0)
                w, h = img.size
                m = Image.new("L", (w, h), 0)
                mask_imgs.append(m)
            else:
                present.append(1.0)
                mask_imgs.append(self._load_mask_as_binary_L(mp))

        # PER-IMAGE CROP SETUP
        left_frac = self.crop_left_frac
        right_frac = self.crop_right_frac

        # If this image has a custom crop, override the defaults
        if stem in self.per_image_crop:
            lf, rf = self.per_image_crop[stem]
            left_frac = lf
            right_frac = rf

        # Additional crop on left/right sides of images to remove black borders
        if (left_frac is not None) and (right_frac is not None) and (left_frac  >= 0.0 or right_frac >= 0.0):
            w, h = img.size
            left_margin = int(left_frac * w)
            right_margin = int(right_frac * w)    # e.g. 5% of width on each side

            # make sure we don't crop away everything
            if left_margin + right_margin < w:
                left   = left_margin
                right  = w - right_margin
                top    = 0
                bottom = h

                img = img.crop((left, top, right, bottom))
                mask_imgs = [m.crop((left, top, right, bottom)) for m in mask_imgs]

        # === AUGMENTATION ===
        if self.augment:
            # ----- TRAINING -----
            if self.patch_mode:
                # Patch-based: random 512×512 crop at *native* resolution
                th, tw = self.image_size  # (H, W)
                W, H = img.size           # PIL: (W, H)

                if H < th or W < tw:
                    # Fallback: if image is somehow smaller, resize up once
                    img = TF.resize(img, self.image_size, interpolation=TF.InterpolationMode.BILINEAR)
                    mask_imgs = [
                        TF.resize(m, self.image_size, interpolation=TF.InterpolationMode.NEAREST)
                        for m in mask_imgs
                    ]
                else:
                    # Randomly choose a 512×512 window (overlaps happen across calls)
                    i, j, h_c, w_c = transforms.RandomCrop.get_params(
                        img, output_size=self.image_size
                    )
                    img = TF.crop(img, i, j, th, tw)
                    mask_imgs = [TF.crop(m, i, j, th, tw) for m in mask_imgs]
            else:
                # Old behaviour: random resized crop over (90–100%) of image, then resize to 512×512
                i, j, h, w = transforms.RandomResizedCrop.get_params(
                    img, scale=self.rr_crop_scale, ratio=self.rr_crop_ratio
                )
                img = TF.resized_crop(
                    img, i, j, h, w, self.image_size,
                    interpolation=TF.InterpolationMode.BILINEAR
                )
                mask_imgs = [
                    TF.resized_crop(
                        m, i, j, h, w, self.image_size,
                        interpolation=TF.InterpolationMode.NEAREST
                    )
                    for m in mask_imgs
                ]

            # Random affine (shared) — same for both modes
            angle = random.uniform(-self.degrees, self.degrees)
            tx = int(self.translate[0] * self.image_size[1])
            ty = int(self.translate[1] * self.image_size[0])
            translate = (random.randint(-tx, tx), random.randint(-ty, ty))
            scale = random.uniform(self.scale_range[0], self.scale_range[1])
            shear = random.uniform(self.shear_range[0], self.shear_range[1])

            img = TF.affine(
                img, angle=angle, translate=translate, scale=scale, shear=shear,
                interpolation=TF.InterpolationMode.BILINEAR, fill=0
            )
            mask_imgs = [
                TF.affine(
                    m, angle=angle, translate=translate, scale=scale, shear=shear,
                    interpolation=TF.InterpolationMode.NEAREST, fill=0
                )
                for m in mask_imgs
            ]

            # Random horizontal flip (shared)
            if random.random() < 0.5:
                img = TF.hflip(img)
                mask_imgs = [TF.hflip(m) for m in mask_imgs]

        else:
            # Validation/Test
            # Keep original behaviour: resize the *whole* FOV to 512×512
            img = TF.resize(img, self.image_size, interpolation=TF.InterpolationMode.BILINEAR)
            mask_imgs = [
                TF.resize(m, self.image_size, interpolation=TF.InterpolationMode.NEAREST)
                for m in mask_imgs
            ]

        # === To tensor ===
        img_t = TF.to_tensor(img)                          # (3,H,W), float32 in [0,1]
        mask_t_list = [TF.pil_to_tensor(m).float() / 255.0 # each -> (1,H,W) in {0,1}
                       for m in mask_imgs]
        masks_t = torch.cat(mask_t_list, dim=0)            # (4,H,W)

        #NEW LINE ADDED
        # Recompute presence AFTER all transforms
        # present_t[c] = 1 if that lesion has any positive pixel in this augmented patch
        present_t = (masks_t.view(masks_t.shape[0], -1).max(dim=1).values > 0).float()

        # Normalize image only
        img_t = self.normalize(img_t)

        # NEW: presence vector as a tensor (4,)
        #present_t = torch.tensor(present, dtype=torch.float32)

        # CHANGE: return presence_t too
        return img_t, masks_t, present_t, stem

'''
"""Visualizing Images"""

import matplotlib.pyplot as plt
# Same mean/std as in RetinaSegDataset
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)

def denormalize(img_tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):
    """
    img_tensor: (3,H,W) after Normalize
    returns: (H,W,3) in [0,1] for plotting
    """
    mean = torch.tensor(mean).view(3, 1, 1)
    std  = torch.tensor(std).view(3, 1, 1)
    img = img_tensor * std + mean
    img = img.clamp(0, 1)
    return img.permute(1, 2, 0).cpu().numpy()

def show_sample(dataset, idx=None, lesion_names=None):
    """
    dataset: e.g., train_ds
    idx: optional fixed index; if None, choose random
    lesion_names: list of names (for titles); default = dataset.lesion_subdirs
    """
    if idx is None:
        idx = np.random.randint(len(dataset))

    img_t, masks_t, present_t, stem = dataset[idx]  # uses your __getitem__
    img_vis = denormalize(img_t)

    if lesion_names is None:
        lesion_names = dataset.lesion_subdirs

    n_lesions = masks_t.shape[0]

    fig, axes = plt.subplots(1, n_lesions + 1, figsize=(4 * (n_lesions + 1), 4))

    # Plot augmented RGB image
    axes[0].imshow(img_vis)
    axes[0].set_title(f"Augmented RGB\n{stem}")
    axes[0].axis("off")

    # Plot each lesion mask overlaid on the RGB image
    for c in range(n_lesions):
        mask = masks_t[c].cpu().numpy()  # (H,W) in {0,1}
        ax = axes[c + 1]
        ax.imshow(img_vis)
        ax.imshow(mask, alpha=0.4)       # semi-transparent mask
        lname = lesion_names[c] if c < len(lesion_names) else f"Lesion {c}"
        ax.set_title(f"{lname}\n present={int(present_t[c].item())}")
        ax.axis("off")

    plt.tight_layout()
    plt.show()

# A version of the dataset with NO augmentation (deterministic)
train_ds_noaug = RetinaSegDataset(
    img_train_folder,
    mask_train_root,
    lesion_subdirs=lesion_subdirs,
    image_size=512,
    augment=False,
    crop_left_frac=0.055,
    crop_right_frac=0.13,
    per_image_crop=per_image_crop# <-- this disables all augmentations
)

for i in [3, 4, 10, 37, 38, 44, 45]:
    show_sample(train_ds_noaug, idx=i, lesion_names=lesion_subdirs)

"""Visualizing Augmented Images"""

# Visualize a few augmented versions of the SAME image
fixed_idx = 0  # pick any index you want

for i in range(3):
    print(f"Augmentation sample {i+1} for index {fixed_idx}")
    show_sample(train_ds, idx=fixed_idx, lesion_names=lesion_subdirs)

'''


"""U-Net Architecture"""

class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )
    def forward(self, x): return self.net(x)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.conv = DoubleConv(in_ch, out_ch)
    def forward(self, x):
        x = self.pool(x)
        return self.conv(x)

class Up(nn.Module):
    def __init__(self, in_ch, skip_ch, out_ch):
        super().__init__()
        # up-convolution halves spatial dims and channels
        self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)
        # after concatenation, channels = (in_ch // 2) + skip_ch
        self.conv = DoubleConv((in_ch // 2) + skip_ch, out_ch)

    def forward(self, x, skip):
        x = self.up(x)
        # handle potential padding differences
        diffY = skip.size(2) - x.size(2)
        diffX = skip.size(3) - x.size(3)
        if diffY or diffX:
            x = F.pad(x, [diffX // 2, diffX - diffX // 2,
                          diffY // 2, diffY - diffY // 2])
        x = torch.cat([skip, x], dim=1)
        return self.conv(x)

class UNetShared(nn.Module):
    """
    Shared encoder-decoder. Separate 1x1 conv heads, one per lesion.
    Call forward(images, lesion_idx) to get a single-channel logit for that lesion.
    """
    def __init__(self, in_ch=3, base=64, lesion_names=("HE","EX","MA","SE")):
        super().__init__()
        self.lesion_names = list(lesion_names)

        # Encoder
        self.inc  = DoubleConv(in_ch, base)
        self.down1 = Down(base, base*2)
        self.down2 = Down(base*2, base*4)
        self.down3 = Down(base*4, base*8)
        self.down4 = Down(base*8, base*8)  # bottleneck keeps same width as previous

        # Decoder
        self.up1 = Up(in_ch=base*8, skip_ch=base*8, out_ch=base*4)  # x5 + x4
        self.up2 = Up(in_ch=base*4, skip_ch=base*4, out_ch=base*2)  # u1 + x3
        self.up3 = Up(in_ch=base*2, skip_ch=base*2, out_ch=base)    # u2 + x2
        self.up4 = Up(in_ch=base,   skip_ch=base,   out_ch=base)    # u3 + x1

        # One head per lesion (1 output channel each)
        self.heads = nn.ModuleDict({
            name: nn.Conv2d(base, 1, kernel_size=1)
            for name in self.lesion_names
        })

    def encode_decode(self, x):
        x1 = self.inc(x)        # 64
        x2 = self.down1(x1)     # 128
        x3 = self.down2(x2)     # 256
        x4 = self.down3(x3)     # 512
        x5 = self.down4(x4)     # 512

        u1 = self.up1(x5, x4)   # 256
        u2 = self.up2(u1, x3)   # 128
        u3 = self.up3(u2, x2)   # 64
        u4 = self.up4(u3, x1)   # 64 (final feature map shared by all heads)
        return u4

    def forward(self, x, lesion_idx: int):
        feats = self.encode_decode(x)
        lesion_name = self.lesion_names[lesion_idx]
        logit = self.heads[lesion_name](feats)  # (B,1,H,W)
        return logit

def dice_loss_from_logits(logits: torch.Tensor, targets: torch.Tensor, eps=1e-6):
    # logits, targets: (B,1,H,W)
    probs = torch.sigmoid(logits)
    num = 2 * (probs * targets).sum(dim=(2,3))
    den = (probs + targets).sum(dim=(2,3)) + eps
    dice_per_sample = 1.0 - (num / den)  # (B,1)
    return dice_per_sample.squeeze(1)     # (B,)

#NEW Weighted BCE Loss Function
def masked_bce_dice(
    logits: torch.Tensor,
    targets: torch.Tensor,
    present_vec: torch.Tensor,
    bce_w: float = 1.0,
    dice_w: float = 1.0,
    use_pos_weight: bool = True,          # <-- weighted BCE on by default
    pos_weight_scale: float = 0.5,        # multiply the computed pos_weight if you want it stronger/weaker
    min_pos_frac: float = 1e-6,           # stability: lower bound on positive fraction
    max_pos_weight: float = 1e3           # cap extreme weights for numerical stability
) -> torch.Tensor:
    """
    logits:      (B,1,H,W) raw scores
    targets:     (B,1,H,W) binary masks in {0,1}
    present_vec: (B,) 1 if this lesion exists in the image, else 0 (sample-level mask)
    Returns:     scalar loss (averaged over *present* samples)
    """
    # ----- BCE (with optional positive-class reweighting) -----
    if use_pos_weight:
        # fraction of positive pixels per sample (B,1,1,1)
        with torch.no_grad():
            p = targets.mean(dim=(2, 3), keepdim=True).clamp_(min=min_pos_frac)
            # pos_weight ≈ (#neg / #pos); broadcasts over H,W
            pos_w = ((1.0 - p) / p) * pos_weight_scale
            pos_w = pos_w.clamp_(max=max_pos_weight)

        bce_per_pixel = F.binary_cross_entropy_with_logits(
            logits, targets, reduction="none", pos_weight=pos_w
        )
    else:
        bce_per_pixel = F.binary_cross_entropy_with_logits(
            logits, targets, reduction="none"
        )

    # average BCE over H,W per sample -> (B,)
    bce_hw = bce_per_pixel.mean(dim=(2, 3)).squeeze(1)

    # ----- Dice loss per sample -> (B,) -----
    # Uses your existing helper; expects (B,1,H,W)
    probs = torch.sigmoid(logits)
    eps = 1e-6
    num = 2 * (probs * targets).sum(dim=(2, 3))
    den = (probs + targets).sum(dim=(2, 3)) + eps
    dice = 1.0 - (num / den)  # (B,1)
    dice = dice.squeeze(1)    # (B,)

    # ----- Blend & mask by 'present' samples -----
    per_sample = bce_w * bce_hw + dice_w * dice  # (B,)
    present_sum = present_vec.sum().clamp(min=1.0)
    return (per_sample * present_vec).sum() / present_sum

from torch.cuda.amp import autocast, GradScaler
import math
from collections import defaultdict
import time

def pixel_accuracy_from_logits(logits, targets):
    """
    Computes pixel-wise accuracy: fraction of pixels correctly predicted.
    logits, targets: (B,1,H,W)
    """
    with torch.no_grad():
        probs = torch.sigmoid(logits)
        preds = (probs > 0.5).float()
        correct = (preds == targets).float().sum()
        total = torch.numel(preds)
        return (correct / total).item()

from sklearn.metrics import average_precision_score
@torch.no_grad()
def _binary_metrics_from_batch_logits(
    logits: torch.Tensor,         # (B,1,H,W) raw logits
    targets: torch.Tensor,        # (B,1,H,W) {0,1}
    present_vec: torch.Tensor,    # (B,) {0,1} -> evaluate only where present==1
    prob_threshold: float = 0.5,
    eps: float = 1e-6,
):
    """
    Returns per-batch sums needed to compute epoch-level Dice/IoU,
    and returns flattened probs/targets for PR-AUC (present-only).
    """
    B = logits.shape[0]
    probs = torch.sigmoid(logits)                 # (B,1,H,W)

    # Thresholded preds for Dice/IoU
    preds = (probs >= prob_threshold).float()     # (B,1,H,W)

    # Mask: only include samples where present==1
    pres = present_vec.view(B, 1, 1, 1)           # (B,1,1,1)

    # Intersection/Union (sum over H,W)
    inter = (preds * targets * pres).sum(dim=(2,3))       # (B,1)
    pred_sum = (preds * pres).sum(dim=(2,3))              # (B,1)
    targ_sum = (targets * pres).sum(dim=(2,3))            # (B,1)
    union = (pred_sum + targ_sum - inter).clamp_min(eps)  # (B,1)

    # Per-sample dice, iou (avoid divide-by-zero with eps)
    dice_num = (2 * inter)
    dice_den = (pred_sum + targ_sum).clamp_min(eps)
    dice = (dice_num / dice_den).squeeze(1)               # (B,)

    iou = (inter / union).squeeze(1)                      # (B,)

    # Collect flattened probs/targets for PR-AUC (only present samples)
    # We flatten over spatial dims and keep only present samples.
    keep = (present_vec > 0.5)                            # (B,)
    probs_flat = probs[keep].reshape(-1).detach().cpu().numpy()
    targs_flat = targets[keep].reshape(-1).detach().cpu().numpy()

    # Sums for epoch aggregation (sum over present samples)
    dice_sum = dice[keep].sum().item()
    iou_sum  = iou[keep].sum().item()
    n_pres_samples = int(keep.sum().item())

    return {
        "dice_sum": dice_sum,
        "iou_sum": iou_sum,
        "n_pres_samples": n_pres_samples,
        "probs_flat": probs_flat,
        "targs_flat": targs_flat,
    }

def train_one_lesion(
    model: UNetShared,
    lesion_idx: int,
    train_loader,
    val_loader,
    device="cuda",
    epochs=1,
    lr=1e-3,
    weight_decay=1e-4,
    bce_w=1.0,
    dice_w=1.0,
    pos_weight_scale = 0.5,
    ckpt_dir: str = "checkpoints_unet_shared",
    lesion_names=("Haemorrhages","Hard Exudates","Microaneurysms","Soft Exudates", "Optic Disc"),
    prob_threshold: float = 0.5,      # for Dice/IoU binarization
    monitor: str = "val_loss",        # or "val_dice" if you prefer
):
    
    os.makedirs(ckpt_dir, exist_ok=True)
    lesion_name = lesion_names[lesion_idx]

    model = model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    scaler = GradScaler(enabled=(device.startswith("cuda") and torch.cuda.is_available()))

    best_score = math.inf if monitor == "val_loss" else -math.inf
    history = defaultdict(list)

    for epoch in range(1, epochs+1):
        print(pos_weight_scale)
        print("\n")
        # ---------------- TRAIN ----------------
        model.train()
        t0 = time.time()
        running = 0.0
        n_batches = 0

        for imgs, masks, present, names in train_loader:
            imgs = imgs.to(device, non_blocking=True)
            target = masks[:, lesion_idx:lesion_idx+1].to(device, non_blocking=True)  # (B,1,H,W)
            pres   = present[:, lesion_idx].to(device, non_blocking=True)             # (B,)

            optimizer.zero_grad(set_to_none=True)
            with autocast(enabled=(device.startswith("cuda") and torch.cuda.is_available())):
                logits = model(imgs, lesion_idx)  # (B,1,H,W)
                loss = masked_bce_dice(logits, target, pres, bce_w=bce_w, dice_w=dice_w, pos_weight_scale=pos_weight_scale)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            running += loss.item()
            n_batches += 1

        train_loss = running / max(1, n_batches)
        history["train_loss"].append(train_loss)

        # ---------------- VALIDATE ----------------
        model.eval()
        v_running, v_batches = 0.0, 0

        # accumulators for metrics
        dice_sum_epoch = 0.0
        iou_sum_epoch  = 0.0
        n_pres_samples_epoch = 0
        pr_probs_all = []
        pr_targs_all = []

        with torch.no_grad():
            for imgs, masks, present, names in val_loader:
                imgs   = imgs.to(device, non_blocking=True)
                target = masks[:, lesion_idx:lesion_idx+1].to(device, non_blocking=True)
                pres   = present[:, lesion_idx].to(device, non_blocking=True)

                with autocast(enabled=(device.startswith("cuda") and torch.cuda.is_available())):
                    logits = model(imgs, lesion_idx)
                    v_loss = masked_bce_dice(logits, target, pres, bce_w=bce_w, dice_w=dice_w, pos_weight_scale=pos_weight_scale)

                v_running += v_loss.item()
                v_batches += 1

                # metrics for this batch
                m = _binary_metrics_from_batch_logits(
                        logits=logits, targets=target, present_vec=pres,
                        prob_threshold=prob_threshold)
                dice_sum_epoch      += m["dice_sum"]
                iou_sum_epoch       += m["iou_sum"]
                n_pres_samples_epoch += m["n_pres_samples"]
                if m["probs_flat"].size > 0:
                    pr_probs_all.append(m["probs_flat"])
                    pr_targs_all.append(m["targs_flat"])

        val_loss = v_running / max(1, v_batches)
        history["val_loss"].append(val_loss)

        # Epoch-level Dice/IoU (mean over present samples)
        if n_pres_samples_epoch > 0:
            val_dice = dice_sum_epoch / n_pres_samples_epoch
            val_iou  = iou_sum_epoch  / n_pres_samples_epoch
        else:
            val_dice = float("nan")
            val_iou  = float("nan")

        # Epoch-level PR-AUC (average precision)
        if len(pr_probs_all) > 0:
            probs_flat = np.concatenate(pr_probs_all, axis=0)
            targs_flat = np.concatenate(pr_targs_all, axis=0)
            try:
                val_ap = average_precision_score(targs_flat, probs_flat)
            except Exception:
                val_ap = float("nan")
        else:
            val_ap = float("nan")

        history["val_dice"].append(val_dice)
        history["val_iou"].append(val_iou)
        history["val_ap"].append(val_ap)

        dt = time.time() - t0
        print(f"[{lesion_name}] Epoch {epoch:02d}/{epochs} | "
              f"train {train_loss:.4f} | val {val_loss:.4f} | "
              f"Dice {val_dice:.4f} | IoU {val_iou:.4f} | AP {val_ap:.4f} | {dt:.1f}s")

        # ---------------- CHECKPOINT ----------------
        score = val_loss if monitor == "val_loss" else val_dice
        is_better = (score < best_score) if monitor == "val_loss" else (score > best_score)

        if is_better:
            best_score = score
            ckpt_path = os.path.join(ckpt_dir, f"unet_shared_{lesion_name}_best.pth")
            torch.save({"model": model.state_dict(),
                        "epoch": epoch,
                        "val_loss": val_loss,
                        "val_dice": val_dice,
                        "val_iou":  val_iou,
                        "val_ap":   val_ap}, ckpt_path)
            print(f"  -> saved {ckpt_path}")

    return history

#MODEL RUN
def run_training():
    per_image_crop = {
    "IDRiD_04": (0.015, 0.085),
    "IDRiD_12": (0.015, 0.085),
    "IDRiD_57": (0.015, 0.085),
    "IDRiD_65": (0.015, 0.085),
    }

    # Your directories
    img_train_folder  = "A. Segmentation/3. WDES_Splits/train/images"
    mask_train_root   = "A. Segmentation/3. WDES_Splits/train/masks"   # contains 4 subfolders

    img_val_folder    = "A. Segmentation/3. WDES_Splits/val/images"
    mask_val_root     = "A. Segmentation/3. WDES_Splits/val/masks"

    img_test_folder   = "A. Segmentation/3. WDES_Splits/test/images"
    mask_test_root    = "A. Segmentation/3. WDES_Splits/test/masks"

    lesion_subdirs = ("Haemorrhages", "Hard Exudates", "Microaneurysms", "Soft Exudates", "Optic Disc")

    train_ds = RetinaSegDataset(
        img_train_folder, mask_train_root, lesion_subdirs=lesion_subdirs,
        image_size=512, augment=True, crop_left_frac=0.055, crop_right_frac=0.13,
        per_image_crop=per_image_crop, patch_mode=True, patches_per_image=8)

    val_ds   = RetinaSegDataset(
        img_val_folder, mask_val_root, lesion_subdirs=lesion_subdirs,
        image_size=512, augment=False, crop_left_frac=0.055, crop_right_frac=0.13,
        per_image_crop=per_image_crop)

    test_ds  = RetinaSegDataset(
        img_test_folder, mask_test_root, lesion_subdirs=lesion_subdirs,
        image_size=512, augment=False, crop_left_frac=0.055, crop_right_frac=0.13,
        per_image_crop=per_image_crop)

    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=4, pin_memory=True)
    val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=4, pin_memory=True)
    test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, num_workers=4, pin_memory=True)

    #checking the lengths of these datasets
    print(len(train_ds))
    print(len(val_ds))
    print(len(test_ds))

    # Lesion order should match your dataset’s channel/order
    lesion_names = ("Haemorrhages", "Hard Exudates", "Microaneurysms", "Soft Exudates", "Optic Disc")
    #lesion_names = ("Haemorrhages")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = UNetShared(in_ch=3, base=64, lesion_names=lesion_names)  # base=64 is a good start for 512x512

    POS_WEIGHT_BY_LESION = {
        "Haemorrhages": 1.0,
        "Hard Exudates": 0.5,
        "Microaneurysms": 1.0,
        "Soft Exudates": 0.25, #changed from 0.5 to 0.25 for better precision hopefully
        "Optic Disc": 0.5
    }

    for lesion_idx, lname in enumerate(lesion_names):
        scale = POS_WEIGHT_BY_LESION[lname]
        print(f"\n=== Training {lname} ({lesion_idx}) ===")
        _hist = train_one_lesion(
            model=model,
            lesion_idx=lesion_idx,
            train_loader=train_loader,
            val_loader=val_loader,
            device=device,
            epochs=1,
            pos_weight_scale = scale
        )

if __name__ == "__main__":
    run_training()

'''
"""Testing"""

# --- Safe loader wrapper for PyTorch 2.6 ---
def load_ckpt_forgiving(path, device):
    try:
        # Try the strict safe load first
        add_safe_globals([np.core.multiarray.scalar])
        return torch.load(path, map_location=device, weights_only=True)
    except Exception:
        # Fallback to old behavior if file was saved before PyTorch 2.6
        return torch.load(path, map_location=device, weights_only=False)

@torch.no_grad()
def evaluate_one_lesion_on_test(
    model_class,
    lesion_idx,
    ckpt_path,
    test_loader,
    device="cuda",
    lesion_names=("Haemorrhages","Hard Exudates","Microaneurysms","Soft Exudates", "Optic Disc"),
    prob_threshold=0.5,
):
    model = model_class(in_ch=3, base=64, lesion_names=lesion_names).to(device)
    ckpt = load_ckpt_forgiving(ckpt_path, device)
    model.load_state_dict(ckpt["model"])
    model.eval()

    lname = lesion_names[lesion_idx]
    print(f"\nEvaluating {lname}...")

    dice_sum = iou_sum = n_samples = 0
    probs_all, targs_all = [], []

    for imgs, masks, present, names in test_loader:
        imgs   = imgs.to(device, non_blocking=True)
        target = masks[:, lesion_idx:lesion_idx+1].to(device)
        pres   = present[:, lesion_idx].to(device)
        logits = model(imgs, lesion_idx)
        probs  = torch.sigmoid(logits)
        preds  = (probs >= prob_threshold).float()
        pres_mask = pres.view(-1,1,1,1)

        inter = (preds * target * pres_mask).sum(dim=(2,3))
        pred_sum = (preds * pres_mask).sum(dim=(2,3))
        targ_sum = (target * pres_mask).sum(dim=(2,3))
        union = (pred_sum + targ_sum - inter).clamp_min(1e-6)
        dice = (2 * inter / (pred_sum + targ_sum + 1e-6)).squeeze(1)
        iou  = (inter / union).squeeze(1)

        dice_sum += dice[pres>0].sum().item()
        iou_sum  += iou[pres>0].sum().item()
        n_samples += int((pres>0).sum().item())

        if (pres>0).any():
            probs_all.append(probs[pres>0].cpu().numpy().ravel())
            targs_all.append(target[pres>0].cpu().numpy().ravel())

    mean_dice = dice_sum / max(1, n_samples)
    mean_iou  = iou_sum / max(1, n_samples)
    if len(probs_all) > 0:
        probs_flat = np.concatenate(probs_all)
        targs_flat = np.concatenate(targs_all)
        aupr = average_precision_score(targs_flat, probs_flat)
    else:
        aupr = float("nan")

    print(f"[{lname}] Dice={mean_dice:.4f} | IoU={mean_iou:.4f} | AUPR={aupr:.4f}")
    return {"dice": mean_dice, "iou": mean_iou, "aupr": aupr}

lesion_names = ("Haemorrhages","Hard Exudates","Microaneurysms","Soft Exudates", "Optic Disc")
device = "cuda" if torch.cuda.is_available() else "cpu"

results = {}
for i, lname in enumerate(lesion_names):
    ckpt = f"checkpoints_unet_shared/unet_shared_{lname}_best.pth"
    results[lname] = evaluate_one_lesion_on_test(
        model_class=UNetShared,
        lesion_idx=i,
        ckpt_path=ckpt,
        test_loader=test_loader,
        device=device,
        lesion_names=lesion_names,
    )

print("\n=== Test Results Summary ===")
for lname, m in results.items():
    print(f"{lname:18s} | Dice {m['dice']:.4f} | IoU {m['iou']:.4f} | AUPR {m['aupr']:.4f}")

IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)

def denormalize(img_t, mean=IMAGENET_MEAN, std=IMAGENET_STD):
    """
    img_t: (3,H,W) tensor after Normalize
    returns: (H,W,3) numpy image in [0,1]
    """
    if isinstance(mean, tuple):
        mean = torch.tensor(mean)
    if isinstance(std, tuple):
        std = torch.tensor(std)

    mean = mean.view(3, 1, 1)
    std  = std.view(3, 1, 1)
    img = img_t * std + mean
    img = img.clamp(0, 1)
    return img.permute(1, 2, 0).cpu().numpy()

"""Visualize Predicted Segmentation Map"""

from torch.serialization import add_safe_globals

# (you already defined this above; include here if needed)
def load_ckpt_forgiving(path, device):
    try:
        add_safe_globals([np.core.multiarray.scalar])
        return torch.load(path, map_location=device, weights_only=True)
    except Exception:
        return torch.load(path, map_location=device, weights_only=False)


@torch.no_grad()
def visualize_single_lesion_prediction(
    model_class,
    ckpt_path: str,
    lesion_names,
    lesion_idx: int,
    dataset,
    sample_idx: int,
    device: str = "cuda",
    prob_threshold: float = 0.5,
):
    """
    Show prediction for ONE lesion on ONE image.

    lesion_idx: index into lesion_names and mask channels (0..len-1)
    dataset: e.g. test_ds or val_ds
    sample_idx: which image in that dataset to visualize
    """
    # 1) Build and load model for this lesion-set
    model = model_class(in_ch=3, base=64, lesion_names=lesion_names).to(device)
    ckpt = load_ckpt_forgiving(ckpt_path, device)
    model.load_state_dict(ckpt["model"])
    model.eval()

    lname = lesion_names[lesion_idx]
    print(f"Visualizing lesion: {lname} (index {lesion_idx})")
    print(f"Checkpoint: {ckpt_path}")
    print(f"Sample index: {sample_idx}")

    # 2) Get one sample from dataset
    img_t, masks_t, present_t, stem = dataset[sample_idx]   # img_t: (3,H,W), masks_t: (C,H,W)
    print(f"Image stem: {stem}")

    # Move to device for inference
    img_in = img_t.unsqueeze(0).to(device)  # (1,3,H,W)

    # 3) Forward pass for THIS lesion only
    logits = model(img_in, lesion_idx)      # (1,1,H,W)
    prob_map = torch.sigmoid(logits)[0, 0].cpu().numpy()  # (H,W)
    pred_mask = (prob_map >= prob_threshold).astype(np.float32)

    # 4) Ground-truth mask for this lesion
    gt_mask = masks_t[lesion_idx].cpu().numpy()  # (H,W)

    # 5) Denormalize image for display
    img_vis = denormalize(img_t.cpu())

    # 6) Plot
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    axes[0].imshow(img_vis)
    axes[0].set_title(f"Original image\n{stem}")
    axes[0].axis("off")

    axes[1].imshow(img_vis)
    axes[1].imshow(gt_mask, alpha=0.5)
    axes[1].set_title(f"GT mask: {lname}")
    axes[1].axis("off")

    axes[2].imshow(img_vis)
    axes[2].imshow(pred_mask, alpha=0.5)
    axes[2].set_title(f"Predicted mask ({lname})\nthreshold = {prob_threshold}")
    axes[2].axis("off")

    plt.tight_layout()
    plt.show()

# Map lesion name -> index
lesion_to_idx = {name: i for i, name in enumerate(lesion_names)}

lesion_name = "Optic Disc"
lesion_idx = lesion_to_idx[lesion_name]

ckpt_path = f"checkpoints_unet_shared/unet_shared_{lesion_name}_best.pth"
sample_idx = 11  # 0-based index into test_ds

visualize_single_lesion_prediction(
    model_class=UNetShared,
    ckpt_path=ckpt_path,
    lesion_names=lesion_names,
    lesion_idx=lesion_idx,
    dataset=test_ds,
    sample_idx=sample_idx,
    device=device,
    prob_threshold=0.5,
)

'''