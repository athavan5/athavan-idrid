# -*- coding: utf-8 -*-
"""IDRiD_SMALL_PATCH_December_3_UNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lst17QG1FQHwu4vxXOrQ3oNqSVzxdQyC
"""


import zipfile
import shutil
from pathlib import Path
from sklearn.model_selection import train_test_split
import random
from PIL import Image
import numpy as np
from collections import defaultdict
import torch
import os
import cv2
import re

import glob
from torchvision import transforms, datasets
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms.functional as TF
import matplotlib.pyplot as plt

#from __future__ import annotations
import math
from dataclasses import dataclass
from typing import Tuple, Dict, Any

import torch.nn as nn
import torch.nn.functional as F

from torch.cuda.amp import autocast, GradScaler
import math
import time
from sklearn.metrics import average_precision_score
from torch.serialization import add_safe_globals

'''
#Open the Zipfile
with zipfile.ZipFile('A. Segmentation.zip', 'r') as zip_ref:
    # List all contents
    file_list = zip_ref.namelist()
    print("Contents of the ZIP file:")
    for file in file_list:
        print(file)
'''

class DoubleConv(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )
    def forward(self, x): return self.net(x)

class Down(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.pool = nn.MaxPool2d(2)
        self.conv = DoubleConv(in_ch, out_ch)
    def forward(self, x):
        x = self.pool(x)
        return self.conv(x)

class Up(nn.Module):
    def __init__(self, in_ch, skip_ch, out_ch):
        super().__init__()
        # up-convolution halves spatial dims and channels
        self.up = nn.ConvTranspose2d(in_ch, in_ch // 2, kernel_size=2, stride=2)
        # after concatenation, channels = (in_ch // 2) + skip_ch
        self.conv = DoubleConv((in_ch // 2) + skip_ch, out_ch)

    def forward(self, x, skip):
        x = self.up(x)
        # handle potential padding differences
        diffY = skip.size(2) - x.size(2)
        diffX = skip.size(3) - x.size(3)
        if diffY or diffX:
            x = F.pad(x, [diffX // 2, diffX - diffX // 2,
                          diffY // 2, diffY - diffY // 2])
        x = torch.cat([skip, x], dim=1)
        return self.conv(x)

class UNetShared(nn.Module):
    """
    Shared encoder-decoder. Separate 1x1 conv heads, one per lesion.
    Call forward(images, lesion_idx) to get a single-channel logit for that lesion.
    """
    def __init__(self, in_ch=3, base=64, lesion_names=("HE","EX","MA","SE")):
        super().__init__()
        self.lesion_names = list(lesion_names)

        # Encoder
        self.inc  = DoubleConv(in_ch, base)
        self.down1 = Down(base, base*2)
        self.down2 = Down(base*2, base*4)
        self.down3 = Down(base*4, base*8)
        self.down4 = Down(base*8, base*8)  # bottleneck keeps same width as previous

        # Decoder
        self.up1 = Up(in_ch=base*8, skip_ch=base*8, out_ch=base*4)  # x5 + x4
        self.up2 = Up(in_ch=base*4, skip_ch=base*4, out_ch=base*2)  # u1 + x3
        self.up3 = Up(in_ch=base*2, skip_ch=base*2, out_ch=base)    # u2 + x2
        self.up4 = Up(in_ch=base,   skip_ch=base,   out_ch=base)    # u3 + x1

        # One head per lesion (1 output channel each)
        self.heads = nn.ModuleDict({
            name: nn.Conv2d(base, 1, kernel_size=1)
            for name in self.lesion_names
        })

    def encode_decode(self, x):
        x1 = self.inc(x)        # 64
        x2 = self.down1(x1)     # 128
        x3 = self.down2(x2)     # 256
        x4 = self.down3(x3)     # 512
        x5 = self.down4(x4)     # 512

        u1 = self.up1(x5, x4)   # 256
        u2 = self.up2(u1, x3)   # 128
        u3 = self.up3(u2, x2)   # 64
        u4 = self.up4(u3, x1)   # 64 (final feature map shared by all heads)
        return u4

    def forward(self, x, lesion_idx: int):
        feats = self.encode_decode(x)
        lesion_name = self.lesion_names[lesion_idx]
        logit = self.heads[lesion_name](feats)  # (B,1,H,W)
        return logit

"""Apply Augmentations"""

class RetinaSegDataset(Dataset):
    """
    Loads one RGB image and its 4 lesion masks as a 4-channel tensor (C=4,H,W).
    If augment=True: applies the SAME random crop/affine/flip to image AND masks.
    """
    def __init__(
        self,
        img_dir: str,
        mask_root: str,
        lesion_subdirs = ("Combined MA", "Combined HE", "Combined SE", "Combined EX", "Combined OD"),
        image_size: int | tuple = 256,
        augment: bool = True,
        degrees: float = 30,
        translate=(0.10, 0.10),
        scale_range=(0.9, 1.1),
        shear_range=(-15, 15),
        rr_crop_scale=(0.9, 1.0),
        rr_crop_ratio=(0.75, 1.333),
        normalize_mean=(0.485, 0.456, 0.406),
        normalize_std=(0.229, 0.224, 0.225),
        crop_left_frac: float = 0.055,
        crop_right_frac: float = 0.013,
        per_image_crop: dict | None = None,

        # JUST ADDED -> patch-based training options
        patch_mode: bool = False,
        patches_per_image: int = 1,
    ):
        self.img_dir = Path(img_dir)
        self.mask_root = Path(mask_root)
        self.lesion_subdirs = lesion_subdirs
        self.image_size = (image_size, image_size) if isinstance(image_size, int) else tuple(image_size)
        self.augment = augment
        self.crop_left_frac = crop_left_frac
        self.crop_right_frac = crop_right_frac
        self.per_image_crop = per_image_crop or {}

        # JUST ADDED -> patch-based options
        self.patch_mode = patch_mode          # if True: thus, random 512×512 patches
        self.patches_per_image = max(1, patches_per_image)

        # augmentation params
        self.degrees = degrees
        self.translate = translate
        self.scale_range = scale_range
        self.shear_range = shear_range
        self.rr_crop_scale = rr_crop_scale
        self.rr_crop_ratio = rr_crop_ratio

        # collect images (any common image extension)
        self.img_paths = sorted(
            sum([list(self.img_dir.glob(f"*.{ext}")) for ext in ["jpg","jpeg","png","tif","tiff","bmp"]], [])
        )
        if len(self.img_paths) == 0:
            raise FileNotFoundError(f"No images found in {self.img_dir}")

        # verify all lesion folders exist
        for sub in self.lesion_subdirs:
            if not (self.mask_root / sub).exists():
                raise FileNotFoundError(f"Missing mask subfolder: {self.mask_root / sub}")

        self.normalize = transforms.Normalize(mean=normalize_mean, std=normalize_std)

    def __len__(self):
        # In patch mode, we conceptually replicate each image P times per epoch.
        if self.patch_mode and self.patches_per_image > 1:
            return len(self.img_paths) * self.patches_per_image
        return len(self.img_paths)

    def _find_mask_path(self, subdir: Path, stem: str):
        # try multiple extensions for masks
        for ext in ("tif","tiff","png","jpg","jpeg","bmp"):
            p = subdir / f"{stem}.{ext}"
            if p.exists():
                return p
        # fallback: glob by stem.*
        hits = glob.glob(str(subdir / f"{stem}.*"))
        if len(hits) > 0:
            return Path(hits[0])
        # CHANGE: return None instead of raising
        return None

    def _load_image(self, p: Path):
        # PIL can read TIFF; for LZW you may need imagecodecs (you already installed earlier)
        return Image.open(p).convert("RGB")

    def _load_mask_as_binary_L(self, p: Path):
        # convert to single-channel and binarize (>0 -> 1)
        m = Image.open(p).convert("L")
        # ensure binary (some masks are 0/255 or multi-gray); threshold at >0
        m = m.point(lambda x: 255 if x > 0 else 0)
        return m

    def __getitem__(self, idx):
        # In patch_mode, many dataset indices map to the same image,
        # but we draw a fresh random patch each time.
        if self.patch_mode:
            img_idx = idx % len(self.img_paths)
        else:
            img_idx = idx

        img_path = self.img_paths[img_idx]
        stem = img_path.stem

        # load image
        img = self._load_image(img_path)

        # load 4 lesion masks in a fixed order
        mask_imgs = []
        present = []
        for sub in self.lesion_subdirs:
            mp = self._find_mask_path(self.mask_root / sub, stem)
            if mp is None:
                # mark missing; make a blank mask same size as the image
                present.append(0.0)
                w, h = img.size
                m = Image.new("L", (w, h), 0)
                mask_imgs.append(m)
            else:
                present.append(1.0)
                mask_imgs.append(self._load_mask_as_binary_L(mp))

        # PER-IMAGE CROP SETUP
        left_frac = self.crop_left_frac
        right_frac = self.crop_right_frac

        # If this image has a custom crop, override the defaults
        if stem in self.per_image_crop:
            lf, rf = self.per_image_crop[stem]
            left_frac = lf
            right_frac = rf

        # Additional crop on left/right sides of images to remove black borders
        if (left_frac is not None) and (right_frac is not None) and (left_frac  >= 0.0 or right_frac >= 0.0):
            w, h = img.size
            left_margin = int(left_frac * w)
            right_margin = int(right_frac * w)    # e.g. 5% of width on each side

            # make sure we don't crop away everything
            if left_margin + right_margin < w:
                left   = left_margin
                right  = w - right_margin
                top    = 0
                bottom = h

                img = img.crop((left, top, right, bottom))
                mask_imgs = [m.crop((left, top, right, bottom)) for m in mask_imgs]

        # === AUGMENTATION ===
        if self.augment:
            # ----- TRAINING -----
            if self.patch_mode:
                # Patch-based: random 512×512 crop at *native* resolution
                th, tw = self.image_size  # (H, W)
                W, H = img.size           # PIL: (W, H)

                if H < th or W < tw:
                    # Fallback: if image is somehow smaller, resize up once
                    img = TF.resize(img, self.image_size, interpolation=TF.InterpolationMode.BILINEAR)
                    mask_imgs = [
                        TF.resize(m, self.image_size, interpolation=TF.InterpolationMode.NEAREST)
                        for m in mask_imgs
                    ]
                else:
                    # Randomly choose a 512×512 window (overlaps happen across calls)
                    i, j, h_c, w_c = transforms.RandomCrop.get_params(
                        img, output_size=self.image_size
                    )
                    img = TF.crop(img, i, j, th, tw)
                    mask_imgs = [TF.crop(m, i, j, th, tw) for m in mask_imgs]
            else:
                # Old behaviour: random resized crop over (90–100%) of image, then resize to 512×512
                i, j, h, w = transforms.RandomResizedCrop.get_params(
                    img, scale=self.rr_crop_scale, ratio=self.rr_crop_ratio
                )
                img = TF.resized_crop(
                    img, i, j, h, w, self.image_size,
                    interpolation=TF.InterpolationMode.BILINEAR
                )
                mask_imgs = [
                    TF.resized_crop(
                        m, i, j, h, w, self.image_size,
                        interpolation=TF.InterpolationMode.NEAREST
                    )
                    for m in mask_imgs
                ]

            # Random affine (shared) — same for both modes
            angle = random.uniform(-self.degrees, self.degrees)
            tx = int(self.translate[0] * self.image_size[1])
            ty = int(self.translate[1] * self.image_size[0])
            translate = (random.randint(-tx, tx), random.randint(-ty, ty))
            scale = random.uniform(self.scale_range[0], self.scale_range[1])
            shear = random.uniform(self.shear_range[0], self.shear_range[1])

            img = TF.affine(
                img, angle=angle, translate=translate, scale=scale, shear=shear,
                interpolation=TF.InterpolationMode.BILINEAR, fill=0
            )
            mask_imgs = [
                TF.affine(
                    m, angle=angle, translate=translate, scale=scale, shear=shear,
                    interpolation=TF.InterpolationMode.NEAREST, fill=0
                )
                for m in mask_imgs
            ]

            # Random horizontal flip (shared)
            if random.random() < 0.5:
                img = TF.hflip(img)
                mask_imgs = [TF.hflip(m) for m in mask_imgs]

        else:
            # Validation/Test
            # Keep original behaviour: resize the *whole* FOV to 512×512
            img = TF.resize(img, self.image_size, interpolation=TF.InterpolationMode.BILINEAR)
            mask_imgs = [
                TF.resize(m, self.image_size, interpolation=TF.InterpolationMode.NEAREST)
                for m in mask_imgs
            ]

        # === To tensor ===
        img_t = TF.to_tensor(img)                          # (3,H,W), float32 in [0,1]
        mask_t_list = [TF.pil_to_tensor(m).float() / 255.0 # each -> (1,H,W) in {0,1}
                       for m in mask_imgs]
        masks_t = torch.cat(mask_t_list, dim=0)            # (4,H,W)

        #NEW LINE ADDED
        # Recompute presence AFTER all transforms
        # present_t[c] = 1 if that lesion has any positive pixel in this augmented patch
        present_t = (masks_t.view(masks_t.shape[0], -1).max(dim=1).values > 0).float()

        # Normalize image only
        img_t = self.normalize(img_t)

        # NEW: presence vector as a tensor (4,)
        #present_t = torch.tensor(present, dtype=torch.float32)

        # CHANGE: return presence_t too
        return img_t, masks_t, present_t, stem

'''
"""Visualizing Images"""

import matplotlib.pyplot as plt
# Same mean/std as in RetinaSegDataset
IMAGENET_MEAN = (0.485, 0.456, 0.406)
IMAGENET_STD  = (0.229, 0.224, 0.225)

def denormalize(img_tensor, mean=IMAGENET_MEAN, std=IMAGENET_STD):
    """
    img_tensor: (3,H,W) after Normalize
    returns: (H,W,3) in [0,1] for plotting
    """
    mean = torch.tensor(mean).view(3, 1, 1)
    std  = torch.tensor(std).view(3, 1, 1)
    img = img_tensor * std + mean
    img = img.clamp(0, 1)
    return img.permute(1, 2, 0).cpu().numpy()

def show_sample(dataset, idx=None, lesion_names=None):
    """
    dataset: e.g., train_ds
    idx: optional fixed index; if None, choose random
    lesion_names: list of names (for titles); default = dataset.lesion_subdirs
    """
    if idx is None:
        idx = np.random.randint(len(dataset))

    img_t, masks_t, present_t, stem = dataset[idx]  # uses your __getitem__
    img_vis = denormalize(img_t)

    if lesion_names is None:
        lesion_names = dataset.lesion_subdirs

    n_lesions = masks_t.shape[0]

    fig, axes = plt.subplots(1, n_lesions + 1, figsize=(4 * (n_lesions + 1), 4))

    # Plot augmented RGB image
    axes[0].imshow(img_vis)
    axes[0].set_title(f"Augmented RGB\n{stem}")
    axes[0].axis("off")

    # Plot each lesion mask overlaid on the RGB image
    for c in range(n_lesions):
        mask = masks_t[c].cpu().numpy()  # (H,W) in {0,1}
        ax = axes[c + 1]
        ax.imshow(img_vis)
        ax.imshow(mask, alpha=0.4)       # semi-transparent mask
        lname = lesion_names[c] if c < len(lesion_names) else f"Lesion {c}"
        ax.set_title(f"{lname}\n present={int(present_t[c].item())}")
        ax.axis("off")

    plt.tight_layout()
    plt.show()

# A version of the dataset with NO augmentation (deterministic)
train_ds_noaug = RetinaSegDataset(
    img_train_folder,
    mask_train_root,
    lesion_subdirs=lesion_subdirs,
    image_size=512,
    augment=False,
    crop_left_frac=0.055,
    crop_right_frac=0.13,
    per_image_crop=per_image_crop# <-- this disables all augmentations
)

for i in [3, 4, 10, 37, 38, 44, 45]:
    show_sample(train_ds_noaug, idx=i, lesion_names=lesion_subdirs)

"""Visualizing Augmented Images"""

# Visualize a few augmented versions of the SAME image
fixed_idx = 0  # pick any index you want

for i in range(3):
    print(f"Augmentation sample {i+1} for index {fixed_idx}")
    show_sample(train_ds, idx=fixed_idx, lesion_names=lesion_subdirs)

'''

#MODEL RUN
def run_training():
    per_image_crop = {
    "IDRiD_04": (0.015, 0.085),
    "IDRiD_12": (0.015, 0.085),
    "IDRiD_57": (0.015, 0.085),
    "IDRiD_65": (0.015, 0.085),
    }

    # Your directories
    img_train_folder  = "A. Segmentation/3. WDES_Splits/train/images"
    mask_train_root   = "A. Segmentation/3. WDES_Splits/train/masks"   # contains 4 subfolders

    img_val_folder    = "A. Segmentation/3. WDES_Splits/val/images"
    mask_val_root     = "A. Segmentation/3. WDES_Splits/val/masks"

    img_test_folder   = "A. Segmentation/3. WDES_Splits/test/images"
    mask_test_root    = "A. Segmentation/3. WDES_Splits/test/masks"

    lesion_subdirs = ("Haemorrhages", "Hard Exudates", "Microaneurysms", "Soft Exudates", "Optic Disc")

    train_ds = RetinaSegDataset(
        img_train_folder, mask_train_root, lesion_subdirs=lesion_subdirs,
        image_size=512, augment=True, crop_left_frac=0.055, crop_right_frac=0.13,
        per_image_crop=per_image_crop, patch_mode=True, patches_per_image=8)

    val_ds   = RetinaSegDataset(
        img_val_folder, mask_val_root, lesion_subdirs=lesion_subdirs,
        image_size=512, augment=False, crop_left_frac=0.055, crop_right_frac=0.13,
        per_image_crop=per_image_crop)

    test_ds  = RetinaSegDataset(
        img_test_folder, mask_test_root, lesion_subdirs=lesion_subdirs,
        image_size=512, augment=False, crop_left_frac=0.055, crop_right_frac=0.13,
        per_image_crop=per_image_crop)

    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True,  num_workers=4, pin_memory=True)
    val_loader   = DataLoader(val_ds,   batch_size=8, shuffle=False, num_workers=4, pin_memory=True)
    test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, num_workers=4, pin_memory=True)

    #checking the lengths of these datasets
    print(len(train_ds))
    print(len(val_ds))
    print(len(test_ds))

    lesion_names = ("Haemorrhages", "Hard Exudates", "Microaneurysms", "Soft Exudates", "Optic Disc")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    IMAGENET_MEAN = (0.485, 0.456, 0.406)
    IMAGENET_STD  = (0.229, 0.224, 0.225)

    def denormalize(img_t, mean=IMAGENET_MEAN, std=IMAGENET_STD):
        """
        img_t: (3,H,W) tensor after Normalize
        returns: (H,W,3) numpy image in [0,1]
        """
        if isinstance(mean, tuple):
            mean = torch.tensor(mean)
        if isinstance(std, tuple):
            std = torch.tensor(std)

        mean = mean.view(3, 1, 1)
        std  = std.view(3, 1, 1)
        img = img_t * std + mean
        img = img.clamp(0, 1)
        return img.permute(1, 2, 0).cpu().numpy()
    
    """Visualize Predicted Segmentation Map"""

    # (you already defined this above; include here if needed)
    def load_ckpt_forgiving(path, device):
        try:
            add_safe_globals([np.core.multiarray.scalar])
            return torch.load(path, map_location=device, weights_only=True)
        except Exception:
            return torch.load(path, map_location=device, weights_only=False)


    @torch.no_grad()
    def visualize_single_lesion_prediction(
        model_class,
        ckpt_path: str,
        lesion_names,
        lesion_idx: int,
        dataset,
        sample_idx: int,
        device: str = "cuda",
        prob_threshold: float = 0.5,
    ):
        """
        Show prediction for ONE lesion on ONE image.

        lesion_idx: index into lesion_names and mask channels (0..len-1)
        dataset: e.g. test_ds or val_ds
        sample_idx: which image in that dataset to visualize
        """
        # 1) Build and load model for this lesion-set
        model = model_class(in_ch=3, base=64, lesion_names=lesion_names).to(device)
        ckpt = load_ckpt_forgiving(ckpt_path, device)
        model.load_state_dict(ckpt["model"])
        model.eval()

        lname = lesion_names[lesion_idx]
        print(f"Visualizing lesion: {lname} (index {lesion_idx})")
        print(f"Checkpoint: {ckpt_path}")
        print(f"Sample index: {sample_idx}")

        # 2) Get one sample from dataset
        img_t, masks_t, present_t, stem = dataset[sample_idx]   # img_t: (3,H,W), masks_t: (C,H,W)
        print(f"Image stem: {stem}")

        # Move to device for inference
        img_in = img_t.unsqueeze(0).to(device)  # (1,3,H,W)

        # 3) Forward pass for THIS lesion only
        logits = model(img_in, lesion_idx)      # (1,1,H,W)
        prob_map = torch.sigmoid(logits)[0, 0].cpu().numpy()  # (H,W)
        pred_mask = (prob_map >= prob_threshold).astype(np.float32)

        # 4) Ground-truth mask for this lesion
        gt_mask = masks_t[lesion_idx].cpu().numpy()  # (H,W)

        # 5) Denormalize image for display
        img_vis = denormalize(img_t.cpu())

        # 6) Plot
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))

        axes[0].imshow(img_vis)
        axes[0].set_title(f"Original image\n{stem}")
        axes[0].axis("off")

        axes[1].imshow(img_vis)
        axes[1].imshow(gt_mask, alpha=0.5)
        axes[1].set_title(f"GT mask: {lname}")
        axes[1].axis("off")

        axes[2].imshow(img_vis)
        axes[2].imshow(pred_mask, alpha=0.5)
        axes[2].set_title(f"Predicted mask ({lname})\nthreshold = {prob_threshold}")
        axes[2].axis("off")

        plt.tight_layout()
        plt.show()

    # Map lesion name -> index
    lesion_to_idx = {name: i for i, name in enumerate(lesion_names)}

    lesion_name = "Optic Disc"
    lesion_idx = lesion_to_idx[lesion_name]

    ckpt_path = f"checkpoints_unet_shared/unet_shared_{lesion_name}_best.pth"
    sample_idx = 11  # 0-based index into test_ds

    visualize_single_lesion_prediction(
        model_class=UNetShared,
        ckpt_path=ckpt_path,
        lesion_names=lesion_names,
        lesion_idx=lesion_idx,
        dataset=test_ds,
        sample_idx=sample_idx,
        device=device,
        prob_threshold=0.5,
    )

if __name__ == "__main__":
    run_training()


